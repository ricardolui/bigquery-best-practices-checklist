{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Best Practices: Ingestion & Storage\n",
    "\n",
    "**Objective:** Optimize how data is ingested and stored in BigQuery. We will analyze storage billing models, data layout (partitioning/clustering), and streaming efficiency.\n",
    "\n",
    "**Contributors:**\n",
    "*   Google Cloud Data Analytics Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize gcloud for authentication in the notebook, if running outside from BigQuery\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bigframes.pandas as bpd\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize BigQuery Client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = client.project  # Uses default project from environment\n",
    "REGION = \"us\" # UPDATE THIS to your dataset region (e.g., eu, us-central1)\n",
    "GEMINI_MODEL_NAME = \"gemini-3-pro-preview\" # Updated to use gemini-3-pro-preview\n",
    "VERTEX_AI_LOCATION = \"global\" # Vertex AI location for Gemini models\n",
    "GEMINI_ENDPOINT_URL = f\"https://aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{VERTEX_AI_LOCATION}/publishers/google/models/{GEMINI_MODEL_NAME}\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Gemini Endpoint: {GEMINI_ENDPOINT_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Pre-requirements\n",
    "\n",
    "This section sets up the necessary prerequisites for the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Create BigQuery dataset\n",
    "\n",
    "Create a BigQuery dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_sql = f\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS `bq_bestpractices_checklist`\n",
    "OPTIONS (location = '{REGION}')\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    job = client.query(create_model_sql)\n",
    "    job.result()\n",
    "    print(\"Dataset created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Create Gemini Model\n",
    "\n",
    "Create a BQML Remote Model that uses the Gemini model via DEFAULT connection for AI-powered recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_sql = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `bq_bestpractices_checklist.gemini`\n",
    "REMOTE WITH CONNECTION DEFAULT\n",
    "OPTIONS (endpoint = '{GEMINI_ENDPOINT_URL}')\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    job = client.query(create_model_sql)\n",
    "    job.result()\n",
    "    print(\"Model created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Create Organization-level View\n",
    "\n",
    "Create a dataset and a view that aggregates jobs from the top 20 projects in the organization. This view will be used as the source for the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_query = \"\"\"\n",
    "EXECUTE IMMEDIATE (\n",
    "  (\n",
    "    SELECT\n",
    "      'CREATE OR REPLACE VIEW `bq_bestpractices_checklist.jobs_by_top_20_projects` AS (' ||\n",
    "      STRING_AGG(\n",
    "        'SELECT * FROM `' || project_id || '.region-us.INFORMATION_SCHEMA.JOBS_BY_PROJECT`',\n",
    "        ' UNION ALL '\n",
    "      ) || ')'\n",
    "    FROM (\n",
    "      SELECT\n",
    "        project_id\n",
    "      FROM\n",
    "        `region-us.INFORMATION_SCHEMA.JOBS_BY_ORGANIZATION`\n",
    "      WHERE \n",
    "        creation_time > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\n",
    "      GROUP BY\n",
    "        1\n",
    "      ORDER BY\n",
    "        SUM(total_slot_ms) DESC\n",
    "      LIMIT\n",
    "        20\n",
    "    )\n",
    "  )\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    job = client.query(setup_query)\n",
    "    job.result()\n",
    "    print(\"View created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating view: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Storage Model Optimization\n",
    "\n",
    "**Objective:** Identify datasets where switching between Logical and Physical storage billing models can save costs.\n",
    "**Methodology:** Compare forecasted costs for both models based on active/long-term storage and compression ratios.\n",
    "**Success Criteria:** Identify and migrate datasets with significant potential savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_storage = f\"\"\"\n",
    "DECLARE active_logical_gib_price FLOAT64 DEFAULT 0.02;\n",
    "DECLARE long_term_logical_gib_price FLOAT64 DEFAULT 0.01;\n",
    "DECLARE active_physical_gib_price FLOAT64 DEFAULT 0.04;\n",
    "DECLARE long_term_physical_gib_price FLOAT64 DEFAULT 0.02;\n",
    "\n",
    "WITH\n",
    " storage_sizes AS (\n",
    "   SELECT\n",
    "     project_id   AS project_id,\n",
    "     table_schema AS dataset_name,\n",
    "     -- Logical\n",
    "     SUM(IF(deleted=false, active_logical_bytes, 0)) / power(1024, 3) AS active_logical_gib,\n",
    "     SUM(IF(deleted=false, long_term_logical_bytes, 0)) / power(1024, 3) AS long_term_logical_gib,\n",
    "     -- Physical\n",
    "     SUM(active_physical_bytes) / power(1024, 3) AS active_physical_gib,\n",
    "     SUM(active_physical_bytes - time_travel_physical_bytes) / power(1024, 3) AS active_no_tt_physical_gib,\n",
    "     SUM(long_term_physical_bytes) / power(1024, 3) AS long_term_physical_gib,\n",
    "     -- Restorable previously deleted physical\n",
    "     SUM(time_travel_physical_bytes) / power(1024, 3) AS time_travel_physical_gib,\n",
    "     SUM(fail_safe_physical_bytes) / power(1024, 3) AS fail_safe_physical_gib,\n",
    "   FROM\n",
    "     `region-{REGION}`.INFORMATION_SCHEMA.TABLE_STORAGE_BY_ORGANIZATION\n",
    "   WHERE total_physical_bytes + fail_safe_physical_bytes > 0\n",
    "     -- Base the forecast on base tables only for highest precision results\n",
    "     AND table_type  = 'BASE TABLE'\n",
    "     GROUP BY 1,2\n",
    " )\n",
    "SELECT\n",
    "  project_id,\n",
    "  dataset_name,\n",
    "  -- Logical\n",
    "  ROUND(active_logical_gib, 2) AS active_logical_gib,\n",
    "  ROUND(long_term_logical_gib, 2) AS long_term_logical_gib,\n",
    "  -- Physical\n",
    "  ROUND(active_physical_gib, 2) AS active_physical_gib,\n",
    "  ROUND(long_term_physical_gib, 2) AS long_term_physical_gib,\n",
    "  ROUND(time_travel_physical_gib, 2) AS time_travel_physical_gib,\n",
    "  ROUND(fail_safe_physical_gib, 2) AS fail_safe_physical_gib,\n",
    "  -- Compression ratio\n",
    "  ROUND(SAFE_DIVIDE(active_logical_gib, active_no_tt_physical_gib), 2) AS active_compression_ratio,\n",
    "  ROUND(SAFE_DIVIDE(long_term_logical_gib, long_term_physical_gib), 2) AS long_term_compression_ratio,\n",
    "  -- Forecast costs logical\n",
    "  ROUND(active_logical_gib * active_logical_gib_price, 2) AS forecast_active_logical_cost,\n",
    "  ROUND(long_term_logical_gib * long_term_logical_gib_price, 2) AS forecast_long_term_logical_cost,\n",
    "  -- Forecast costs physical\n",
    "  ROUND((active_no_tt_physical_gib + time_travel_physical_gib + fail_safe_physical_gib) * active_physical_gib_price, 2) AS forecast_active_physical_cost,\n",
    "  ROUND(long_term_physical_gib * long_term_physical_gib_price, 2) AS forecast_long_term_physical_cost,\n",
    "  -- Forecast costs total\n",
    "  ROUND(((active_logical_gib * active_logical_gib_price) + (long_term_logical_gib * long_term_logical_gib_price)) -\n",
    "     (((active_no_tt_physical_gib + time_travel_physical_gib + fail_safe_physical_gib) * active_physical_gib_price) + (long_term_physical_gib * long_term_physical_gib_price)), 2) AS forecast_total_cost_difference\n",
    "FROM\n",
    "  storage_sizes\n",
    "ORDER BY\n",
    "  (forecast_active_logical_cost + forecast_active_physical_cost) DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_storage = client.query(query_storage).to_dataframe()\n",
    "df_storage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for significant differences\n",
    "df_significant_difference = df_storage[df_storage['forecast_total_cost_difference'].abs() > 0.00]\n",
    "df_significant_difference = df_significant_difference.sort_values(by='forecast_total_cost_difference', ascending=False)\n",
    "print(\"Datasets with potential cost differences (sorted by difference):\")\n",
    "df_significant_difference.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "\n",
    "for index, row in df_significant_difference.iterrows():\n",
    "    dataset_name = row['dataset_name']\n",
    "    project_id = row['project_id']\n",
    "    forecast_total_cost_difference = row['forecast_total_cost_difference']\n",
    "\n",
    "    # Determine recommendation based on which model is cheaper\n",
    "    # A positive forecast_total_cost_difference means logical is more expensive than physical\n",
    "    # A negative forecast_total_cost_difference means physical is more expensive than logical\n",
    "    if forecast_total_cost_difference > 0:\n",
    "        recommended_billing_model = 'PHYSICAL'\n",
    "    elif forecast_total_cost_difference < 0:\n",
    "        recommended_billing_model = 'LOGICAL'\n",
    "    else:\n",
    "        recommended_billing_model = 'NO_CHANGE_COST_SAME'\n",
    "\n",
    "    recommendations.append({\n",
    "        'project_id' : project_id,\n",
    "        'dataset_name': dataset_name,\n",
    "        'recommended_billing_model': recommended_billing_model,\n",
    "        'forecast_total_cost_difference': forecast_total_cost_difference\n",
    "    })\n",
    "\n",
    "df_storage_recommendations = pd.DataFrame(recommendations)\n",
    "print(\"Recommended billing models for datasets:\")\n",
    "df_storage_recommendations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_commands = []\n",
    "\n",
    "for index, row in df_storage_recommendations.iterrows():\n",
    "    dataset_name = row['dataset_name']\n",
    "    recommended_model = row['recommended_billing_model']\n",
    "    project = row['project_id']\n",
    "\n",
    "    if recommended_model in ['LOGICAL', 'PHYSICAL']:\n",
    "        command = f\"ALTER SCHEMA `{project}.{dataset_name}` SET OPTIONS (storage_billing_model = '{recommended_model}');\"\n",
    "        sql_commands.append(command)\n",
    "\n",
    "print(\"--Generated SQL commands for manual execution:\\n\")\n",
    "for cmd in sql_commands:\n",
    "    print(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation:** Review the generated `ALTER SCHEMA` commands and execute them to switch billing models for cost savings.\n",
    "\n",
    "**WARNING:** If you would like to implement the previous suggestions, run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "print(\"Generated SQL commands for manual execution:\\n\")\n",
    "for cmd in sql_commands:\n",
    "    print(cmd)\n",
    "\n",
    "# Executes the commands\n",
    "# Initialize BigQuery Client (ensure this is done once, but re-init here for self-contained cell)\n",
    "try:\n",
    "    client\n",
    "except NameError:\n",
    "    client = bigquery.Client()\n",
    "\n",
    "print(\"Executing SQL commands dynamically...\")\n",
    "for i, cmd in enumerate(sql_commands):\n",
    "    print(f\"\\nExecuting command {i+1}/{len(sql_commands)}: {cmd}\")\n",
    "    try:\n",
    "        query_job = client.query(cmd)\n",
    "        query_job.result() # Waits for the job to complete\n",
    "        print(f\"Command {i+1} executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing command {i+1}: {e}\")\n",
    "\n",
    "print(\"\\nDynamic execution complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Layout: Detecting Date-Sharded Tables\n",
    "\n",
    "**Objective:** Identify legacy date-sharded tables (e.g., `events_20240101`).\n",
    "**Methodology:** Regex match on table names in `TABLE_STORAGE_BY_ORGANIZATION`.\n",
    "**Success Criteria:** Migrate these tables to **Native Partitioning** for better performance and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sharding = f\"\"\"\n",
    "SELECT\n",
    "    table_catalog as project_id,\n",
    "    table_schema as dataset_id,\n",
    "    REGEXP_EXTRACT(table_name, r'(.*)_\\\\d{{8}}$') AS base_table_name,\n",
    "    COUNT(*) as shard_count,\n",
    "    SUM(total_rows) as total_rows,\n",
    "    SUM(total_logical_bytes) / 1024 / 1024 / 1024 as total_logical_gb\n",
    "FROM\n",
    "    `region-{REGION}.INFORMATION_SCHEMA.TABLE_STORAGE_BY_ORGANIZATION`\n",
    "WHERE\n",
    "    REGEXP_CONTAINS(table_name, r'_\\\\d{{8}}$')\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "HAVING\n",
    "    shard_count > 5\n",
    "ORDER BY\n",
    "    total_logical_gb DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_sharding = client.query(query_sharding).to_dataframe()\n",
    "print(\"Top Date-Sharded Tables Candidates for Partitioning:\")\n",
    "df_sharding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation:** Convert sharded tables to partitioned tables to improve query performance and reduce metadata overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Optimization\n",
    "\n",
    "**Objective:** Detect high usage of legacy streaming inserts.\n",
    "**Methodology:** Analyze `STREAMING_TIMELINE_BY_ORGANIZATION` for legacy requests.\n",
    "**Success Criteria:** Migrate to **Storage Write API** for lower costs and higher throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_streaming = f\"\"\"\n",
    "SELECT\n",
    "  project_id,\n",
    "  dataset_id,\n",
    "  table_id,\n",
    "  SUM(total_requests) AS total_legacy_requests,\n",
    "  SUM(total_input_bytes) AS total_legacy_bytes\n",
    "FROM\n",
    "  `region-{REGION}`.INFORMATION_SCHEMA.STREAMING_TIMELINE_BY_ORGANIZATION\n",
    "WHERE\n",
    "  start_timestamp > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\n",
    "GROUP BY\n",
    "  1, 2, 3\n",
    "ORDER BY\n",
    "  total_legacy_bytes DESC;\n",
    "\"\"\"\n",
    "\n",
    "df_streaming = client.query(query_streaming).to_dataframe()\n",
    "print(\"Top Tables using Legacy Streaming:\")\n",
    "df_streaming.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation:** If `total_legacy_bytes` is high, consider switching to the BigQuery Storage Write API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AI-Powered Recommendations\n",
    "\n",
    "**Objective:** Use Generative AI to synthesize findings.\n",
    "**Methodology:** Summarize findings and use Gemini Pro via BQML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Summarize findings for Gemini\n",
    "summary_text = f\"\"\"\n",
    "Audit Findings for Ingestion & Storage:\n",
    "1. Storage Model: Found {len(df_storage_recommendations)} datasets where switching billing models could save costs.\n",
    "2. Data Layout: Identified {len(df_sharding)} date-sharded tables that should be partitioned.\n",
    "3. Streaming: Found {len(df_streaming)} tables using legacy streaming inserts.\n",
    "\"\"\"\n",
    "\n",
    "# Construct the prompt\n",
    "prompt = f\"Analyze the following metrics regarding BigQuery Ingestion & Storage and provide 3 actionable recommendations. Context: {summary_text}\"\n",
    "\n",
    "# Call Gemini via BQML\n",
    "query_gemini = f\"\"\"\n",
    "SELECT\n",
    "  ml_generate_text_result['candidates'][0]['content'] AS recommendation\n",
    "FROM\n",
    "  ML.GENERATE_TEXT(\n",
    "    MODEL `bq_bestpractices_checklist.gemini`,\n",
    "    (SELECT '''{prompt}''' AS prompt),\n",
    "    STRUCT(\n",
    "      0.2 AS temperature,\n",
    "      8192 AS max_output_tokens\n",
    "    )\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_gemini = client.query(query_gemini).to_dataframe()\n",
    "    print(\"AI-Powered Recommendations:\")\n",
    "    for index, row in df_gemini.iterrows():\n",
    "        try:\n",
    "          # Parse the JSON string from Gemini\n",
    "          recommendation_data = json.loads(row['recommendation'])\n",
    "          \n",
    "          # Extract the text content\n",
    "          if 'parts' in recommendation_data and recommendation_data['parts']:\n",
    "              text_content = recommendation_data['parts'][0]['text']\n",
    "              display(Markdown(text_content))\n",
    "          else:\n",
    "              # Fallback if structure is different\n",
    "              print(row['recommendation'])\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback if not valid JSON\n",
    "            print(row['recommendation'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying recommendation: {e}\")\n",
    "except Exception as e:\n",
    "    print(\"Error calling Gemini. Ensure the model exists and you have permissions.\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
