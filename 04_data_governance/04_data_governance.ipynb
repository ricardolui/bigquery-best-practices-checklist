{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNz_7idNEdlE"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXbw-R0ZGiWf"
      },
      "source": [
        "# Data Governance Validation in BigQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cGtn8TvG7SB"
      },
      "source": [
        "## Overview\n",
        "This notebook demonstrates how to verify **Data Governance** topics related to **BigQuery** Data in GCP by checking IAM permissions and Policies, and **Dataplex Universal Catalog** resources that exists in the project defined in the parameter **PROJECT_ID**.\n",
        "\n",
        "**Dataplex Universal Catalog** replaces **Data Catalog**, providing more complex metadata management, advanced data governance features, more powerful data search, less complex access control, and a unified API to ease development.\n",
        "\n",
        "The **Dataplex Universal Catalog** resources currently supported are: **Business Glossary (Glossary, GlossaryCategory, GlossaryTerm), and Entry Groups, Aspect Types, Entry Types and Entry(s)**.\n",
        "\n",
        "Finally, this notebook showcases how to leverage `bigframes` to perform basic data quality tasks directly within BigQuery by using SQL.\n",
        "\n",
        "**Data quality** is a critical aspect of any data-driven initiative, especially in machine learning and analytics. Poor data quality can lead to inaccurate insights, flawed models, and unreliable decisions. This notebook showcases a customized and manual approach to:\n",
        "\n",
        "1.  **Data Profiling**: Understanding the structure, content, and quality of your data through descriptive statistics.\n",
        "2.  **Anomaly Detection**: Identifying unusual or suspicious data points that deviate from expected patterns.\n",
        "\n",
        "However it is strongly recommended that you can start using **Data Quality** and **Data Profling** Scans to avoid the use of extensive Python and SQL programming. **Dataplex Universal Catalog** lets you better understand the profile of your data by creating a **data profile scan**. A **data profile scan** is a type of **Dataplex Universal Catalog data scan** that analyzes a BigQuery table to generate statistical insights.\n",
        "\n",
        "A **Dataplex Universal Catalog data scan** can be any of the following types:\n",
        "\n",
        "1. **DATA_SCAN_TYPE_UNSPECIFIED:** The data scan type is unspecified.\n",
        "2. **DATA_QUALITY:** Data quality scan.\n",
        "3. **DATA_PROFILE:** Data profile scan.\n",
        "4. **DATA_DISCOVERY:** Data discovery scan.\n",
        "5. **DATA_DOCUMENTATION:** Data documentation scan."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contributors:\n",
        "\n",
        "**HAZBLEYDI CERVERA**, LATAM Data Analytics Regional Team, hazbleydi@google.com"
      ],
      "metadata": {
        "id": "m3CNRHI1Vtww"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "# Environment setup\n",
        "\n",
        "Complete the tasks in this section to set up your environment.\n",
        "\n",
        "**Please carefully review every single cell before run it and provide the required parameters if apply**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq7zKYWelRQP"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Click here](https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com,bigqueryconnection.googleapis.com,cloudfunctions.googleapis.com,run.googleapis.com,artifactregistry.googleapis.com,cloudbuild.googleapis.com,cloudresourcemanager.googleapis.com) to enable the following APIs:\n",
        "\n",
        "  * BigQuery API\n",
        "  * BigQuery Connection API\n",
        "\n",
        "4. If you are running this notebook locally, install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set the PARAMETERS for the Notebook\n",
        "\n",
        "If you don't know your project ID, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe:\n",
        "# uuid: CF01CDDB-7A6B-4787-96BD-2FF7F493A261\n",
        "# output_variable:\n",
        "# config_str:\n",
        "\n",
        "import google.colabsqlviz.explore_dataframe as _vizcell\n",
        "_vizcell.explore_dataframe(df_or_df_name='', uuid='CF01CDDB-7A6B-4787-96BD-2FF7F493A261')"
      ],
      "metadata": {
        "colab_type": "viz",
        "id": "F3tpcIEOxGB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "oM1iC_MfAts1",
        "outputId": "824a15b3-ec9d-4da8-b10b-1ef34f30920c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764903674656,
          "user_tz": 300,
          "elapsed": 1956,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"prj-dataplex-hcv\"  # @param {type:\"string\"}\n",
        "LOCATION = \"US\"\n",
        "DATASET_ID = \"imdb_raw\"\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation"
      ],
      "metadata": {
        "id": "eohJOoKdeN04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bigframes  --quiet\n",
        "!pip install google-cloud-datacatalog  --quiet\n",
        "!pip install --upgrade google-cloud-bigquery-datapolicies --quiet"
      ],
      "metadata": {
        "id": "Ou_-YbXWeWhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764793604742,
          "user_tz": 300,
          "elapsed": 6450,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "efeea26b-1f45-4e26-ef8d-9ee0952bfa55"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/226.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m225.3/226.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m226.6/226.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only\n",
        "\n",
        "Uncomment and run the following cell to restart the kernel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "JgnQWOJgjtbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bigframes.pandas as bf\n",
        "import bigframes.bigquery as bq\n",
        "from google.cloud import bigquery  # Still useful for some direct BQ client operations like INFORMATION_SCHEMA\n",
        "import pandas as pd  # For local display if needed\n",
        "import datetime # For timestamp comparisons"
      ],
      "metadata": {
        "id": "gJlPSGoCg0FR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764903684438,
          "user_tz": 300,
          "elapsed": 798,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IAM Policy Verification\n",
        " This script checks the IAM policy for a specified BigQuery dataset, flagging if any group or user has an overly permissive role like **'roles/bigquery.admin' or 'roles/bigquery.dataOwner'**. IAM Policy Verification (Dataset Level)"
      ],
      "metadata": {
        "id": "cZuFwQqhzbu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the content of a Dataset in BigQuery\n",
        "List of the tables that a dataset contains.\n",
        "The dataset is referenced in the variable: **DATASET_ID**"
      ],
      "metadata": {
        "id": "iPi7cAh0KaJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client()\n",
        "\n",
        "# TODO(developer): Set dataset_id to the ID of the dataset to fetch.\n",
        "dataset_id = f\"{PROJECT_ID}.{DATASET_ID}\"\n",
        "\n",
        "dataset = client.get_dataset(dataset_id)  # Make an API request.\n",
        "\n",
        "full_dataset_id = \"{}.{}\".format(dataset.project, dataset.dataset_id)\n",
        "friendly_name = dataset.friendly_name\n",
        "print(\n",
        "    \"Got dataset '{}' with friendly_name '{}'.\".format(\n",
        "        full_dataset_id, friendly_name\n",
        "    )\n",
        ")\n",
        "\n",
        "# View dataset properties.\n",
        "print(\"Description: {}\".format(dataset.description))\n",
        "print(\"Labels:\")\n",
        "labels = dataset.labels\n",
        "if labels:\n",
        "    for label, value in labels.items():\n",
        "        print(\"\\t{}: {}\".format(label, value))\n",
        "else:\n",
        "    print(\"\\tDataset has no labels defined.\")\n",
        "\n",
        "# View tables in dataset.\n",
        "print(\"Tables:\")\n",
        "tables = list(client.list_tables(dataset))  # Make an API request(s).\n",
        "if tables:\n",
        "    for table in tables:\n",
        "        print(\"\\t{}\".format(table.table_id))\n",
        "else:\n",
        "    print(\"\\tThis dataset does not contain any tables.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NmGYD9_C0sT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764878803308,
          "user_tz": 300,
          "elapsed": 684,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "764b3df2-300e-452c-daf3-de5654848e30"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got dataset 'prj-dataplex-hcv.imdb_raw' with friendly_name 'None'.\n",
            "Description: None\n",
            "Labels:\n",
            "\tgoog-dataplex-project-id: prj-dataplex-hcv\n",
            "\tgoog-dataplex-lake-id: movie-lake\n",
            "\tgoog-dataplex-zone-id: imdb-raw-zone\n",
            "\tgoog-dataplex-asset-id: raw-movie-dataset\n",
            "Tables:\n",
            "\tepisode\n",
            "\tnames\n",
            "\tprincipals\n",
            "\ttitle_basics\n",
            "\ttitle_ratings\n",
            "\ttop_rated_horror_movies\n",
            "\ttop_rated_horror_movies_with_cast_crew_details\n",
            "\ttop_rated_movies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Dataset] Get the IAM Policy of a Dataset\n",
        "Checks a BigQuery dataset's Access Control List.  The dataset is defined previously with the parameter **DATASET_ID**.\n",
        "\n",
        "You can provide access to a dataset by granting an **IAM principal** a predefined or custom role that determines what the principal can do with the dataset. This is also known as attaching an **allow policy** to a resource. After granting access, you can view the dataset's access controls, and you can revoke access to the dataset.\n",
        "\n",
        "For more information on access controls for BigQuery Datasets, see here -> https://docs.cloud.google.com/bigquery/docs/control-access-to-resources-iam"
      ],
      "metadata": {
        "id": "jjyXRakDMomr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud.bigquery import AccessEntry\n",
        "\n",
        "def verify_dataset_iam_policy(project_id: str, dataset_id: str):\n",
        "    \"\"\"\n",
        "    Checks a dataset's Access Control List (ACL).\n",
        "    \"\"\"\n",
        "    print(f\"--- Checking ACL for Dataset: {dataset_id} ---\")\n",
        "    bq_client = bigquery.Client()\n",
        "    dataset_ref = bq_client.dataset(dataset_id, project=project_id)\n",
        "\n",
        "    try:\n",
        "        dataset = bq_client.get_dataset(dataset_ref)\n",
        "        access_entries = dataset.access_entries\n",
        "\n",
        "        fail_count = 0\n",
        "\n",
        "        for entry in access_entries:\n",
        "            print(f\"Access controls for dataset '{dataset_id}':\")\n",
        "            print(f\"  Role: {entry.role}, Entity Type: {entry.entity_type}, Entity ID: {entry.entity_id}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ERROR: Could not retrieve dataset policy: {e}\")\n",
        "\n",
        "# Execute the function\n",
        "verify_dataset_iam_policy(PROJECT_ID, DATASET_ID)"
      ],
      "metadata": {
        "id": "GZuHfz-H0f_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764919730431,
          "user_tz": 300,
          "elapsed": 921,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d8147d01-acf7-4c8d-a6c0-04078b7a6b7f"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Checking ACL for Dataset: demo_dataset ---\n",
            "Access controls for dataset 'demo_dataset':\n",
            "  Role: WRITER, Entity Type: specialGroup, Entity ID: projectWriters\n",
            "Access controls for dataset 'demo_dataset':\n",
            "  Role: OWNER, Entity Type: specialGroup, Entity ID: projectOwners\n",
            "Access controls for dataset 'demo_dataset':\n",
            "  Role: OWNER, Entity Type: userByEmail, Entity ID: admin@hazbleydi.altostrat.com\n",
            "Access controls for dataset 'demo_dataset':\n",
            "  Role: READER, Entity Type: specialGroup, Entity ID: projectReaders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Table] Get the IAM Policy (privileges/roles) assigned to a BigQuery table\n",
        "Retrieves and displays the **IAM policy (privileges/roles)** assigned to a BigQuery table, safely handling both simple and conditional bindings.\n",
        "\n",
        "You need to specify the table as a parameter using the variable **TABLE_ID**."
      ],
      "metadata": {
        "id": "VWrguERCNa9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Configuration --- ###\n",
        "# TODO(developer): Set TABLE_ID to the TABLE in the dataset to fetch.\n",
        "TABLE_ID = \"episode\"\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.api_core import exceptions as gcp_exceptions\n",
        "\n",
        "# Function\n",
        "def get_table_iam_policy(project_id: str, dataset_id: str, table_id: str):\n",
        "    \"\"\"\n",
        "    Retrieves and displays the IAM policy (privileges/roles) assigned to a BigQuery table,\n",
        "    safely handling both simple and conditional bindings.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"--- Checking IAM Policy for Table: {project_id}.{dataset_id}.{table_id} ---\")\n",
        "\n",
        "    try:\n",
        "        # Initialize the BigQuery client\n",
        "        bq_client = bigquery.Client(project=project_id)\n",
        "\n",
        "        # 1. Get the reference to the table\n",
        "        table_ref = bq_client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "        # 2. Retrieve the IAM policy for the table resource\n",
        "        policy = bq_client.get_iam_policy(table_ref)\n",
        "\n",
        "        if not policy.bindings:\n",
        "            print(\"âœ… PASS: No explicit IAM roles are assigned directly to this table.\")\n",
        "            print(\"    (Privileges are likely inherited from the Dataset or Project level.)\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nPolicy Version: {policy.version}, Etag: {policy.etag}\\n\")\n",
        "        print(\"ðŸ›‘ WARNING: Roles listed below are DIRECTLY assigned to the table.\")\n",
        "        print(\"------------------------------------------------------------------\")\n",
        "\n",
        "        # 3. Iterate through bindings (Role -> Members) and display privileges\n",
        "        for binding in policy.bindings:\n",
        "\n",
        "            # Safely get role, members, and condition regardless of whether\n",
        "            # 'binding' is a Policy.Binding object or a raw dict (for conditional bindings).\n",
        "            if isinstance(binding, dict):\n",
        "                # Use safe dictionary access for conditional bindings\n",
        "                role = binding.get('role')\n",
        "                members = binding.get('members')\n",
        "                condition = binding.get('condition', None)\n",
        "            else:\n",
        "                # Assume standard object structure (Policy.Binding) and use attribute access\n",
        "                # NOTE: This is safe because if it fails, the object doesn't contain the expected data.\n",
        "                role = getattr(binding, 'role', None)\n",
        "                members = getattr(binding, 'members', [])\n",
        "                condition = getattr(binding, 'condition', None)\n",
        "\n",
        "            if not role:\n",
        "                print(f\"âš ï¸ UNEXPECTED BINDING STRUCTURE: Could not find 'role' in binding: {binding}\")\n",
        "                continue\n",
        "\n",
        "            # The 'role' is the privilege assigned (e.g., 'roles/bigquery.dataViewer')\n",
        "\n",
        "            print(f\"â–¶ï¸ **Role (Privilege):** **{role}**\")\n",
        "            print(\"   Assigned to Members:\")\n",
        "\n",
        "            for member in members:\n",
        "                print(f\"   - {member}\")\n",
        "\n",
        "            if condition:\n",
        "                # Display the condition details if present (handles both dict and object structures)\n",
        "                title = condition.get('title') if isinstance(condition, dict) else getattr(condition, 'title', 'N/A')\n",
        "                expression = condition.get('expression') if isinstance(condition, dict) else getattr(condition, 'expression', 'N/A')\n",
        "                print(f\"   *Condition:* {title} (Expression: {expression})\")\n",
        "\n",
        "        print(\"\\n------------------------------------------------------------------\")\n",
        "\n",
        "    except gcp_exceptions.NotFound:\n",
        "        print(f\"âŒ ERROR: Table '{table_id}' not found in dataset '{dataset_id}'. Check the IDs.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ERROR: An unexpected error occurred while fetching the IAM policy: {e}\")\n",
        "\n",
        "\n",
        "# Execute the function\n",
        "get_table_iam_policy(PROJECT_ID, DATASET_ID, TABLE_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHJB9c40IQCw",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764879033855,
          "user_tz": 300,
          "elapsed": 647,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d28d2964-9b33-44dc-82ff-ab955ecce020"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Checking IAM Policy for Table: prj-dataplex-hcv.imdb_raw.episode ---\n",
            "\n",
            "Policy Version: 1, Etag: BwZFEr8q0A0=\n",
            "\n",
            "ðŸ›‘ WARNING: Roles listed below are DIRECTLY assigned to the table.\n",
            "------------------------------------------------------------------\n",
            "â–¶ï¸ **Role (Privilege):** **roles/bigquery.filteredDataViewer**\n",
            "   Assigned to Members:\n",
            "   - user:hazbleydi@google.com\n",
            "\n",
            "------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Catalog Verification\n",
        " This script checks the **Dataplex Universal Catalog** resources that exists in the project defined in the parameter **PROJECT_ID**.\n",
        "\n",
        " **Dataplex Universal Catalog** replaces **Data Catalog**, providing more complex metadata management, advanced data governance features, more powerful data search, less complex access control, and a unified API to ease development.\n",
        "\n",
        " The **Dataplex Universal Catalog** resources currently supported are: **Business Glossary (Glossary, GlossaryCategory, GlossaryTerm), and Entry Groups, Aspect Types, Entry Types and Entry(s)**.\n",
        "\n",
        " For more information about **Dataplex Universal Catalog**, please see here -> https://docs.cloud.google.com/dataplex/docs/catalog-overview"
      ],
      "metadata": {
        "id": "X3ghhQUqi3qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calls a Google Cloud REST API using the current users credentials.\n",
        "# This is a utility function, please don't modify it\n",
        "def restAPIHelper(url: str, http_verb: str, request_body: str) -> str:\n",
        "  \"\"\"Calls the Google Cloud REST API passing in the current users credentials\"\"\"\n",
        "\n",
        "  import google.auth.transport.requests\n",
        "  import requests\n",
        "  import google.auth\n",
        "  import json\n",
        "\n",
        "  # Get an access token based upon the current user\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\" : \"application/json\",\n",
        "    \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  if http_verb == \"GET\":\n",
        "    response = requests.get(url, headers=headers)\n",
        "  elif http_verb == \"POST\":\n",
        "    response = requests.post(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PUT\":\n",
        "    response = requests.put(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PATCH\":\n",
        "    response = requests.patch(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"DELETE\":\n",
        "    response = requests.delete(url, headers=headers)\n",
        "  else:\n",
        "    raise RuntimeError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    return json.loads(response.content)\n",
        "    #image_data = json.loads(response.content)[\"predictions\"][0][\"bytesBase64Encoded\"]\n",
        "  else:\n",
        "    error = f\"Error restAPIHelper -> ' Status: '{response.status_code}' Text: '{response.text}'\"\n",
        "    raise RuntimeError(error)"
      ],
      "metadata": {
        "id": "sUcl81N0dWWi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764879125126,
          "user_tz": 300,
          "elapsed": 657,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON utility function\n",
        "# This is a utility function, please don't modify it\n",
        "def PrettyPrintJson(json_string):\n",
        "  json_object = json.loads(json_string)\n",
        "  json_formatted_str = json.dumps(json_object, indent=2)\n",
        "  return json_formatted_str"
      ],
      "metadata": {
        "id": "fhkxolOXfnLF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764879132313,
          "user_tz": 300,
          "elapsed": 648,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List Business Glossaries\n",
        "Lists the **Business Glossary** resources for the project **PROJECT_ID**.\n",
        "\n",
        "**Dataplex Universal Catalog Business Glossary** helps streamline data discovery and reduce ambiguity, resulting in better governance, more accurate analysis, and faster insights\n",
        "\n",
        "For more information about **Business Glossaries**, please see here -> https://docs.cloud.google.com/dataplex/docs/manage-glossaries"
      ],
      "metadata": {
        "id": "_v-2MA-ik3ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Configuration --- ###\n",
        "# TODO(developer): Set the variable GLOSSARY_LOCATION.  A glossary location can be different from the BigQuery dataset location.\n",
        "GLOSSARY_LOCATION = \"us\"\n",
        "\n",
        "# Function\n",
        "def existsingGlossaries(parent_path: str):\n",
        "  \"\"\"Checks to see if any Glossary already exists\"\"\"\n",
        "\n",
        "  # https://dataplex.googleapis.com/v1/{parent=projects/*/locations/*}/glossaries\n",
        "  url = f\"https://dataplex.googleapis.com/v1/{parent_path}/glossaries\"\n",
        "\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "\n",
        "  total_glossaries = 0\n",
        "  if \"glossaries\" in json_result:\n",
        "      for item in json_result[\"glossaries\"]:\n",
        "          total_glossaries += 1\n",
        "      print(f\"existsGlossary (GET) json_result: {PrettyPrintJson(json.dumps(json_result))}\")\n",
        "\n",
        "\n",
        "  if total_glossaries > 0:\n",
        "    print(f\"There are {total_glossaries} business glossaries created in the project {PROJECT_ID}.\")\n",
        "  else:\n",
        "    print(\"There are no business glossaries created in the project\")\n",
        "\n",
        "# Execute the function\n",
        "existsingGlossaries(f\"projects/{PROJECT_ID}/locations/{GLOSSARY_LOCATION}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70CaLR06lfrQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764879423213,
          "user_tz": 300,
          "elapsed": 638,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "cd7e6100-89cf-4e82-d94c-0fda6356cc57"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "existsGlossary (GET) json_result: {\n",
            "  \"glossaries\": [\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2\",\n",
            "      \"uid\": \"e4ecaa4c-887d-4146-bf1d-625649fc6207\",\n",
            "      \"displayName\": \"glosario-1\",\n",
            "      \"createTime\": \"2025-12-04T04:42:45.316804088Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:42:46.916365637Z\",\n",
            "      \"termCount\": 2,\n",
            "      \"categoryCount\": 2,\n",
            "      \"etag\": \"EXA8-8si2WZSyADWAtN9mw1mXk_P0-kLz8MDAELN4Hk\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/flgnijmmk-d2c1d5c4-941cdc36-ec618ab4\",\n",
            "      \"uid\": \"50ba4239-79ef-4ab4-9bc6-b1ee297ec824\",\n",
            "      \"displayName\": \"glosario-2-demo\",\n",
            "      \"createTime\": \"2025-12-04T04:47:26.670286938Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:47:28.004315785Z\",\n",
            "      \"termCount\": 1,\n",
            "      \"categoryCount\": 1,\n",
            "      \"etag\": \"l81oMal3T_1TVZE5hSS0Jap98vxjNSCLkD-2zbDJUGo\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "There are 2 business glossaries created in the project prj-dataplex-hcv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List Glossary Categories\n",
        "Lists the detail of **Glossary categories** for the project **PROJECT_ID**.\n",
        "\n",
        "A category lets you organize and structure various categories and terms. **Categories** are defined within a **business glossary**. You can nest categories up to three levels.\n",
        "\n",
        "For more information about **Business Glossaries**, see here -> https://docs.cloud.google.com/dataplex/docs/manage-glossaries"
      ],
      "metadata": {
        "id": "_XegP7xVtDiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Configuration --- ###\n",
        "# TODO(developer): Set the variable GLOSSARY_CATEGORY_LOCATION.  A glossary location can be different from the BigQuery dataset location.\n",
        "# A glossary category has the same location as its parent glossary.\n",
        "GLOSSARY_CATEGORY_LOCATION = \"us\"\n",
        "\n",
        "# Function\n",
        "def listGlossaryCategories(parent_path: str):\n",
        "    \"\"\"List the detail of the Glossary Categories if exists\"\"\"\n",
        "\n",
        "    # 1. First, list all glossaries\n",
        "    # API Documentation: https://dataplex.googleapis.com/v1/{parent=projects/*/locations/*}/glossaries\n",
        "    url = f\"https://dataplex.googleapis.com/v1/{parent_path}/glossaries\"\n",
        "\n",
        "    json_result = restAPIHelper(url, \"GET\", None)\n",
        "\n",
        "    total_categories = 0\n",
        "    if \"glossaries\" in json_result:\n",
        "        for item in json_result[\"glossaries\"]:\n",
        "\n",
        "            full_glossary_name = item[\"name\"]\n",
        "            display_name = item.get(\"displayName\", \"N/A\")\n",
        "            print(f\"ðŸ‘‰ Found Glossary: **{display_name}**\")\n",
        "\n",
        "            # Extract the actual glossary ID from the end of the full name.\n",
        "            # Example: 'projects/.../glossaries/my_glossary_1' -> 'my_glossary_1'\n",
        "            glossary_id = full_glossary_name.split('/')[-1]\n",
        "\n",
        "            # 2. Second, call the GetGlossary API to try and get category details\n",
        "            # The correct endpoint for CATEGORIES would be /glossaries/{glossary_id}/categories\n",
        "\n",
        "            url2 = f\"https://dataplex.googleapis.com/v1/{parent_path}/glossaries/{glossary_id}/categories\"\n",
        "\n",
        "            # Fetch the categories:\n",
        "            # Re-calculating the parent path for the Categories endpoint,\n",
        "            # which is the full glossary name.\n",
        "            categories_parent_path = full_glossary_name\n",
        "            url_categories = f\"https://dataplex.googleapis.com/v1/{categories_parent_path}/categories\"\n",
        "\n",
        "            json_result_categories = restAPIHelper(url_categories, \"GET\", None)\n",
        "\n",
        "            # Check if categories exist in the response\n",
        "            if \"categories\" in json_result_categories and len(json_result_categories[\"categories\"]) > 0:\n",
        "                 print(f\"Existing Categories for Glossary {display_name} (GET) json_result: {PrettyPrintJson(json.dumps(json_result_categories))}\")\n",
        "                 total_categories += len(json_result_categories[\"categories\"])\n",
        "            else:\n",
        "                 print(f\"No Categories found for Glossary: {display_name}\")\n",
        "\n",
        "    if total_categories > 0:\n",
        "        print(f\"There are {total_categories} categories in total across all glossaries created in the project {PROJECT_ID}.\")\n",
        "    else:\n",
        "        print(\"There are no categories in the glossaries created in the project.\")\n",
        "\n",
        "# Execute the function\n",
        "listGlossaryCategories(f\"projects/{PROJECT_ID}/locations/{GLOSSARY_CATEGORY_LOCATION}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvRACVXvwJgm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764879617220,
          "user_tz": 300,
          "elapsed": 626,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c73f12a5-9007-417e-bf81-c1f918e36efc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ‘‰ Found Glossary: **glosario-1**\n",
            "Existing Categories for Glossary glosario-1 (GET) json_result: {\n",
            "  \"categories\": [\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2/categories/jkpeoaigg-8234e3d9-ae1d4c1c-1db63885\",\n",
            "      \"uid\": \"0b79b7d7-313f-4e2e-9052-2b4f72536899\",\n",
            "      \"displayName\": \"Categoria-2\",\n",
            "      \"createTime\": \"2025-12-04T04:43:43.281679Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:43:43.281679Z\",\n",
            "      \"parent\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2/categories/cghfnckob-c072d593-8de33ce7-f61c1c97\",\n",
            "      \"uid\": \"597204b2-e266-4ebf-8134-2b5b53b1dea2\",\n",
            "      \"displayName\": \"Categoria-1\",\n",
            "      \"createTime\": \"2025-12-04T04:43:15.311775Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:43:15.311775Z\",\n",
            "      \"parent\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "ðŸ‘‰ Found Glossary: **glosario-2-demo**\n",
            "Existing Categories for Glossary glosario-2-demo (GET) json_result: {\n",
            "  \"categories\": [\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/flgnijmmk-d2c1d5c4-941cdc36-ec618ab4/categories/ikljeacpk-2a7aa925-8d5401b7-4dcc5bfe\",\n",
            "      \"uid\": \"0c2d28e0-97c3-41f3-995b-41c524de1010\",\n",
            "      \"displayName\": \"Categoria-glosario-2-1\",\n",
            "      \"createTime\": \"2025-12-04T04:47:51.747935Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:47:51.747935Z\",\n",
            "      \"parent\": \"projects/prj-dataplex-hcv/locations/us/glossaries/flgnijmmk-d2c1d5c4-941cdc36-ec618ab4\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "There are 3 categories in total across all glossaries created in the project prj-dataplex-hcv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List Terms in a Glossary Category\n",
        "Lists the detail of the terms that exist in a category of a glossary in the project **PROJECT_ID**.\n",
        "\n",
        "A **term** (short for \"glossary term\") describes a concept that's used in a particular branch of business within your company. For example, the marketing department of a company might create a term that describes cost per click. **Terms** are defined within a **business glossary**, either in the glossary directly or within any category found in the glossary.\n",
        "\n",
        "For more information about **Business Glossaries**, see here -> https://docs.cloud.google.com/dataplex/docs/manage-glossaries"
      ],
      "metadata": {
        "id": "-3yvAyK0zHAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Configuration --- ###\n",
        "# TODO(developer): Set the variable GLOSSARY_CATEGORY_TERMS_LOCATION.  A glossary location can be different from the BigQuery dataset location.\n",
        "# All the terms in a glossary category has the same location as its parent glossary.\n",
        "GLOSSARY_CATEGORY_TERMS_LOCATION = \"us\"\n",
        "\n",
        "# Function\n",
        "def listGlossaryCategoryTerms(parent_path: str):\n",
        "    \"\"\"\n",
        "    Lists the glossaries, their categories, and their terms\n",
        "    within the specified parent path (projects/*/locations/*).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. API Call: List all glossaries\n",
        "    url_glossaries = f\"https://dataplex.googleapis.com/v1/{parent_path}/glossaries\"\n",
        "    json_result_glossaries = restAPIHelper(url_glossaries, \"GET\", None)\n",
        "\n",
        "    total_categories = 0\n",
        "    total_terms = 0\n",
        "\n",
        "    if \"glossaries\" in json_result_glossaries:\n",
        "        print(\"--- Found Glossaries ---\")\n",
        "        for item in json_result_glossaries[\"glossaries\"]:\n",
        "\n",
        "            # Extract Glossary Info\n",
        "            display_name = item.get(\"displayName\", \"N/A\")\n",
        "            full_glossary_name = item[\"name\"] # e.g., projects/.../glossaries/my_glossary_id\n",
        "            glossary_id = full_glossary_name.split('/')[-1]\n",
        "\n",
        "            print(f\"\\nâœ¨ Glossary: **{display_name}** (ID: {glossary_id})\")\n",
        "\n",
        "            # --- API Call 2: List Categories for the current Glossary ---\n",
        "            categories_parent_path = full_glossary_name\n",
        "            url_categories = f\"https://dataplex.googleapis.com/v1/{categories_parent_path}/categories\"\n",
        "\n",
        "            json_result_categories = restAPIHelper(url_categories, \"GET\", None)\n",
        "\n",
        "            if \"categories\" in json_result_categories:\n",
        "                 category_count = len(json_result_categories[\"categories\"])\n",
        "                 total_categories += category_count\n",
        "                 print(f\"  - Categories Found: {category_count}\")\n",
        "                 print(f\"  - Categories Details: {PrettyPrintJson(json.dumps(json_result_categories))}\")\n",
        "            else:\n",
        "                 print(\"  - Categories Found: 0\")\n",
        "\n",
        "            # List Terms for the current Glossary ---\n",
        "            # The API path for terms uses the full glossary name as the parent.\n",
        "            terms_parent_path = full_glossary_name\n",
        "            url_terms = f\"https://dataplex.googleapis.com/v1/{terms_parent_path}/terms\"\n",
        "\n",
        "            json_result_terms = restAPIHelper(url_terms, \"GET\", None)\n",
        "\n",
        "            if \"terms\" in json_result_terms:\n",
        "                 term_count = len(json_result_terms[\"terms\"])\n",
        "                 total_terms += term_count\n",
        "                 print(f\"  - Terms Found: {term_count}\")\n",
        "                 print(f\"  - Terms Details: {PrettyPrintJson(json.dumps(json_result_terms))}\")\n",
        "            else:\n",
        "                 print(\"  - Terms Found: 0\")\n",
        "\n",
        "        print(\"\\n--- Summary ---\")\n",
        "\n",
        "    else:\n",
        "        print(f\"No glossaries found in the project: {parent_path}\")\n",
        "\n",
        "\n",
        "    if total_categories > 0 or total_terms > 0:\n",
        "        print(f\"Total Categories found across all glossaries: {total_categories}\")\n",
        "        print(f\"Total Terms found across all glossaries: {total_terms}\")\n",
        "    else:\n",
        "        print(\"There were no categories or terms found in any glossaries created in the project {PROJECT_ID}.\")\n",
        "\n",
        "# Execute the function\n",
        "listGlossaryCategoryTerms(f\"projects/{PROJECT_ID}/locations/{GLOSSARY_CATEGORY_TERMS_LOCATION}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w29k_CYazeOI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764879702731,
          "user_tz": 300,
          "elapsed": 1333,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ea88aa21-59dc-4996-b500-15b9b3df4827"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Found Glossaries ---\n",
            "\n",
            "âœ¨ Glossary: **glosario-1** (ID: clfnonnmk-775383de-8101f98d-768e2fe2)\n",
            "  - Categories Found: 2\n",
            "  - Categories Details: {\n",
            "  \"categories\": [\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2/categories/jkpeoaigg-8234e3d9-ae1d4c1c-1db63885\",\n",
            "      \"uid\": \"0b79b7d7-313f-4e2e-9052-2b4f72536899\",\n",
            "      \"displayName\": \"Categoria-2\",\n",
            "      \"createTime\": \"2025-12-04T04:43:43.281679Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:43:43.281679Z\",\n",
            "      \"parent\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2/categories/cghfnckob-c072d593-8de33ce7-f61c1c97\",\n",
            "      \"uid\": \"597204b2-e266-4ebf-8134-2b5b53b1dea2\",\n",
            "      \"displayName\": \"Categoria-1\",\n",
            "      \"createTime\": \"2025-12-04T04:43:15.311775Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:43:15.311775Z\",\n",
            "      \"parent\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "  - Terms Found: 2\n",
            "  - Terms Details: {\n",
            "  \"terms\": [\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2/terms/ljjiffifn-2477fbd9-a82740f5-bf679a20\",\n",
            "      \"uid\": \"a1d428f1-1f9f-4685-80b5-5e4beebd8bf3\",\n",
            "      \"displayName\": \"Termino-2\",\n",
            "      \"createTime\": \"2025-12-04T04:43:54.455455Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:43:54.455455Z\",\n",
            "      \"parent\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2/categories/jkpeoaigg-8234e3d9-ae1d4c1c-1db63885\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2/terms/pmdjmidaa-73116acb-9d6d5b89-d6aa6cc5\",\n",
            "      \"uid\": \"7db99a44-a956-4277-946d-295528724c5f\",\n",
            "      \"displayName\": \"Termino-1\",\n",
            "      \"createTime\": \"2025-12-04T04:43:28.376191Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:43:28.376191Z\",\n",
            "      \"parent\": \"projects/prj-dataplex-hcv/locations/us/glossaries/clfnonnmk-775383de-8101f98d-768e2fe2/categories/cghfnckob-c072d593-8de33ce7-f61c1c97\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "âœ¨ Glossary: **glosario-2-demo** (ID: flgnijmmk-d2c1d5c4-941cdc36-ec618ab4)\n",
            "  - Categories Found: 1\n",
            "  - Categories Details: {\n",
            "  \"categories\": [\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/flgnijmmk-d2c1d5c4-941cdc36-ec618ab4/categories/ikljeacpk-2a7aa925-8d5401b7-4dcc5bfe\",\n",
            "      \"uid\": \"0c2d28e0-97c3-41f3-995b-41c524de1010\",\n",
            "      \"displayName\": \"Categoria-glosario-2-1\",\n",
            "      \"createTime\": \"2025-12-04T04:47:51.747935Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:47:51.747935Z\",\n",
            "      \"parent\": \"projects/prj-dataplex-hcv/locations/us/glossaries/flgnijmmk-d2c1d5c4-941cdc36-ec618ab4\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "  - Terms Found: 1\n",
            "  - Terms Details: {\n",
            "  \"terms\": [\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us/glossaries/flgnijmmk-d2c1d5c4-941cdc36-ec618ab4/terms/ekggpgmlm-1e375cd3-89837b34-d9859952\",\n",
            "      \"uid\": \"a9d5374f-392c-4e61-a4a5-923785216ad8\",\n",
            "      \"displayName\": \"Termino-1-glosario2\",\n",
            "      \"createTime\": \"2025-12-04T04:48:04.889471Z\",\n",
            "      \"updateTime\": \"2025-12-04T04:48:04.889471Z\",\n",
            "      \"parent\": \"projects/prj-dataplex-hcv/locations/us/glossaries/flgnijmmk-d2c1d5c4-941cdc36-ec618ab4/categories/ikljeacpk-2a7aa925-8d5401b7-4dcc5bfe\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "--- Summary ---\n",
            "Total Categories found across all glossaries: 3\n",
            "Total Terms found across all glossaries: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metadata Management in Dataplex Universal Catalog\n",
        " **Dataplex Universal Catalog** provides a central platform to store, manage, and access the metadata of your GCP Project. The following sections describe the metadata management features of **Dataplex Universal Catalog**."
      ],
      "metadata": {
        "id": "JWhI_RYnhAUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metadata management in Dataplex Universal Catalog\n",
        "from IPython.display import Image\n",
        "\n",
        "# Print the title/description\n",
        "print(\"Aspect types and entry types relationship in Dataplex Universal Catalog\\n\")\n",
        "print(\"For more information about Dataplex Universal Catalog, please see here -> https://docs.cloud.google.com/dataplex/docs/catalog-overview\\n\")\n",
        "Image(url='https://docs.cloud.google.com/static/dataplex/images/aspect-entry-type.png', width=1200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "sxeHfGgCg2M6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764887851983,
          "user_tz": 300,
          "elapsed": 626,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "f5cac9af-d582-42af-bf38-a6c1b2260b57"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aspect types and entry types relationship in Dataplex Universal Catalog\n",
            "\n",
            "For more information about Dataplex Universal Catalog, please see here -> https://docs.cloud.google.com/dataplex/docs/catalog-overview\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://docs.cloud.google.com/static/dataplex/images/aspect-entry-type.png\" width=\"1200\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List Aspect Types\n",
        "Lists AspectType resources in a Data Catalog for the project **PROJECT_ID**.\n",
        "\n",
        "An **aspect type** is a reusable template for aspects. An **aspect** is a set of related metadata fields that lets you capture metadata within entries to provide meaningful context. You can use aspects to store:\n",
        "\n",
        "**Business metadata:** Information that provides business context, such as\n",
        "data classification.\n",
        "\n",
        "**Technical metadata:** Technical details about the data asset, for example, its schema.\n",
        "\n",
        "**Data-derived metadata:** Information generated from the data itself, such as statistics from a BigQuery table.\n",
        "\n",
        "For more information about **Aspect Types**, see here -> https://docs.cloud.google.com/dataplex/docs/enrich-entries-metadata#aspect-types\n",
        "\n"
      ],
      "metadata": {
        "id": "ZECXI77g3w7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Configuration --- ###\n",
        "# TODO(developer): Set the variable aspectTypeLocation.\n",
        "# An Aspect Type is a resource in a Data Catalog whose location can be different from the BigQuery dataset location.\n",
        "aspectTypeLocation = \"global\"\n",
        "\n",
        "# Function\n",
        "def listsAspectType(parent_path: str):\n",
        "    \"\"\"Checks to see if an Aspect Type already exists\"\"\"\n",
        "\n",
        "    url = f\"https://dataplex.googleapis.com/v1/{parent_path}/aspectTypes\"\n",
        "\n",
        "    json_result = restAPIHelper(url, \"GET\", None)\n",
        "    #print(f\"existsAspectType (GET) json_result: {json_result}\")\n",
        "    total_aspectTypes = 0\n",
        "\n",
        "    # Test to see if exists, if so return\n",
        "    if \"aspectTypes\" in json_result:\n",
        "        for item in json_result[\"aspectTypes\"]:\n",
        "            total_aspectTypes += 1\n",
        "            aspectTypeId = item[\"name\"].split(\"/\")[-1]\n",
        "\n",
        "        print(f\"existsAspectType (GET) json_result: {PrettyPrintJson(json.dumps(json_result))}\")\n",
        "\n",
        "        if total_aspectTypes > 0:\n",
        "            print(f\"There are {total_aspectTypes} aspect type(s) created in the Data Catalog for the project {PROJECT_ID}.\")\n",
        "    else:\n",
        "        print(\"There are no aspect types created in the Data Catalog of this location.\")\n",
        "\n",
        "# Execute the function\n",
        "listsAspectType(f\"projects/{PROJECT_ID}/locations/{aspectTypeLocation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LobXv1W4Pl-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764904264019,
          "user_tz": 300,
          "elapsed": 316,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fc3adc3c-2039-43bb-db21-77381298e7ff"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are no aspect types created in the Data Catalog of this location.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List Entry Types\n",
        "Lists **EntryType** resources in a Data Catalog for the project **PROJECT_ID**.\n",
        "\n",
        "An **entry type** is a template for creating entries in the catalog. An entry represents a data asset, for example a BigQuery table named **test-project.sales_data.customer_orders** is represented as an entry.\n",
        "\n",
        "For more information about **Entry Types**, please see here -> https://docs.cloud.google.com/dataplex/docs/ingest-custom-sources#entry-types"
      ],
      "metadata": {
        "id": "T_VDtq5R8kdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Configuration --- ###\n",
        "# TODO(developer): Set the variable entryTypeLocation.\n",
        "# An Entry Type is a resource in a Data Catalog whose location can be different from the BigQuery dataset location.\n",
        "entryTypeLocation = \"global\"\n",
        "\n",
        "# Function\n",
        "def listsEntryType(parent_path: str):\n",
        "    \"\"\"Checks to see if an Entry Type already exists\"\"\"\n",
        "\n",
        "    # https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.entryTypes/list\n",
        "    url = f\"https://dataplex.googleapis.com/v1/{parent_path}/entryTypes\"\n",
        "\n",
        "\n",
        "    json_result = restAPIHelper(url, \"GET\", None)\n",
        "    total_entryTypes = 0\n",
        "\n",
        "    # Test to see if exists, if so return\n",
        "    if \"entryTypes\" in json_result:\n",
        "        for item in json_result[\"entryTypes\"]:\n",
        "            total_entryTypes += 1\n",
        "            entryTypeId = item[\"name\"].split(\"/\")[-1]\n",
        "\n",
        "        print(f\"entryType (GET) json_result: {PrettyPrintJson(json.dumps(json_result))}\")\n",
        "\n",
        "        if total_entryTypes > 0:\n",
        "            print(f\"There are {total_entryTypes} entry type(s) created in the Data Catalog for the project {PROJECT_ID}.\")\n",
        "    else:\n",
        "        print(\"There are no entry types created in the Data Catalog of this location.\")\n",
        "\n",
        "# Execute the function\n",
        "listsEntryType(f\"projects/{PROJECT_ID}/locations/{entryTypeLocation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz1dV5RIZ24w",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764904307936,
          "user_tz": 300,
          "elapsed": 554,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "563231b1-7344-49eb-da26-9d1add2e0108"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are no entry types created in the Data Catalog of this location.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List Entry Groups\n",
        "Lists **EntryGroups** resources in a Data Catalog for the project **PROJECT_ID**.\n",
        "\n",
        "An **entry group** is a container for one or more entries. You can use entry groups to manage access control and regional location for the entries. Every **entry group** belongs to a project.\n",
        "\n",
        "For more information about **entry groups**, please see here -> https://docs.cloud.google.com/dataplex/docs/ingest-custom-sources#entry-groups"
      ],
      "metadata": {
        "id": "_Vk-9Z8iuwd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Configuration --- ###\n",
        "# TODO(developer): Set the variable entryGroupLocation.\n",
        "# An Entry Group is a resource in a Data Catalog whose location can be different from the BigQuery dataset location.\n",
        "entryGroupLocation = \"global\"\n",
        "\n",
        "# Function\n",
        "def listsEntryGroups(parent_path: str):\n",
        "    \"\"\"Checks to see if an Entry Type already exists\"\"\"\n",
        "\n",
        "    # https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.entryGroups/list\n",
        "    url = f\"https://dataplex.googleapis.com/v1/{parent_path}/entryGroups\"\n",
        "\n",
        "    json_result = restAPIHelper(url, \"GET\", None)\n",
        "    total_entryGroups = 0\n",
        "\n",
        "    # Test to see if exists, if so return\n",
        "    if \"entryGroups\" in json_result:\n",
        "        for item in json_result[\"entryGroups\"]:\n",
        "            total_entryGroups += 1\n",
        "            entryGroupId = item[\"name\"].split(\"/\")[-1]\n",
        "\n",
        "        print(f\"entryGroup (GET) json_result: {PrettyPrintJson(json.dumps(json_result))}\")\n",
        "\n",
        "        if total_entryGroups > 0:\n",
        "            print(f\"There are {total_entryGroups} entry Group(s) created in the Data Catalog for the project {PROJECT_ID}.\")\n",
        "    else:\n",
        "        print(\"There are no entry Groups created in the Data Catalog of this location.\")\n",
        "\n",
        "# Execute the function\n",
        "listsEntryGroups(f\"projects/{PROJECT_ID}/locations/{entryGroupLocation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkyLC_ytySku",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764904397511,
          "user_tz": 300,
          "elapsed": 370,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7b8e5070-f590-4208-acd7-36284e1a914d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entryGroup (GET) json_result: {\n",
            "  \"entryGroups\": [\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/global/entryGroups/@analyticshub\",\n",
            "      \"uid\": \"ab94135a-55c0-470b-86e3-8a91b224f439\",\n",
            "      \"createTime\": \"2024-06-06T18:46:50.208486904Z\",\n",
            "      \"updateTime\": \"2024-06-06T18:46:53.250786983Z\",\n",
            "      \"etag\": \"R0AN6Bv9uLPHDyIEQC2z3lNZNONv1B9D4-3XQGH9Fec\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/global/entryGroups/@storage\",\n",
            "      \"uid\": \"7df0b2a7-c78a-4143-bf4d-96d142cb120f\",\n",
            "      \"createTime\": \"2024-06-07T12:24:53.281182442Z\",\n",
            "      \"updateTime\": \"2024-06-07T12:24:56.467112441Z\",\n",
            "      \"etag\": \"BI05kWpcRhWo1Htxo7SMWZzd3RHUbJqOFqKN_m1OQnQ\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/global/entryGroups/@dataprocmetastore\",\n",
            "      \"uid\": \"beb0be60-9160-452e-8f2e-555edf6e61ac\",\n",
            "      \"createTime\": \"2024-06-07T09:17:52.304669782Z\",\n",
            "      \"updateTime\": \"2024-06-07T09:17:55.613838877Z\",\n",
            "      \"etag\": \"47slZlbea4_Fp0SKgXkKf6ZbDkzvXrZDbnkqjSujpWY\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/global/entryGroups/@spanner\",\n",
            "      \"uid\": \"c8137e23-ee52-4f7d-907f-a2cfd259dc98\",\n",
            "      \"createTime\": \"2024-06-06T17:46:13.529594784Z\",\n",
            "      \"updateTime\": \"2024-06-06T17:46:16.988233589Z\",\n",
            "      \"etag\": \"4QYhNlDEhtwUu9ZRHBJT6e80eB29OaD5yHoI_wUVyoY\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/global/entryGroups/@bigquery\",\n",
            "      \"uid\": \"fe95a0e3-0b38-4309-83e9-d30bf53cf4ab\",\n",
            "      \"createTime\": \"2024-06-07T04:28:32.004910891Z\",\n",
            "      \"updateTime\": \"2024-06-07T04:28:34.763094837Z\",\n",
            "      \"etag\": \"RJD87Fic1wLHNSKrEIwohXi_HYbk2xMcSWOs4CC4qZA\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/global/entryGroups/@pubsub\",\n",
            "      \"uid\": \"04bfc937-8151-46cc-8c1b-f932ea0b29db\",\n",
            "      \"createTime\": \"2024-06-07T06:13:51.833902168Z\",\n",
            "      \"updateTime\": \"2024-06-07T06:13:54.837781559Z\",\n",
            "      \"etag\": \"X-TrvorwkUseEYFBDNMytPhnkFyHSNcSGWl1x0oH20o\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/global/entryGroups/@vertexai\",\n",
            "      \"uid\": \"71882738-23d6-4f3a-9f5a-a42e2729f417\",\n",
            "      \"createTime\": \"2024-06-07T02:04:22.107489135Z\",\n",
            "      \"updateTime\": \"2024-06-07T02:04:25.391836116Z\",\n",
            "      \"etag\": \"tYJDb4No0p3VowauBkdZKMqkC-C-levyTKdmz2-YJy8\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/global/entryGroups/@bigtable\",\n",
            "      \"uid\": \"fa7d90c5-e676-4ea5-9418-d89265d5c6eb\",\n",
            "      \"createTime\": \"2024-06-07T15:23:46.729664763Z\",\n",
            "      \"updateTime\": \"2024-06-07T15:23:50.789104036Z\",\n",
            "      \"etag\": \"sqO-igBYgMEh-4EeZLjZnVe5MBqIyLFMnozEqu2Cq60\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "There are 8 entry Group(s) created in the Data Catalog for the project prj-dataplex-hcv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Profiling\n",
        " **Data profiling** is the process of examining the data available in an information source and collecting statistics and information about that data. It helps us understand the structure, content, and quality of our dataset.\n",
        "\n",
        " **This section also uses SQL to run a data profile for a table in BigQuery**, however for a deeper data profile analysis, it is **strongly recommended** to use the the **data profile scan module** available in the **Dataplex Universal Catalog**. Please see here -> https://docs.cloud.google.com/dataplex/docs/data-profiling-overview"
      ],
      "metadata": {
        "id": "-e5IQBpWJl_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List Dataplex Universal Catalog Data Scans\n",
        "Tests to see if a **Data Scan** exists in the project **PROJECT_ID**\n",
        "\n",
        "**Dataplex Universal Catalog** lets you better understand the profile of your data by creating a **data profile scan**. A **data profile scan** is a type of **Dataplex Universal Catalog data scan** that analyzes a BigQuery table to generate statistical insights.\n",
        "\n",
        "A **Dataplex Universal Catalog data scan** can be any of the following types:\n",
        "\n",
        "*   **DATA_SCAN_TYPE_UNSPECIFIED:** The data scan type is unspecified.\n",
        "*   **DATA_QUALITY:** Data quality scan.\n",
        "*   **DATA_PROFILE:** Data profile scan.\n",
        "*   **DATA_DISCOVERY:** Data discovery scan.\n",
        "*   **DATA_DOCUMENTATION:** Data documentation scan.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E3qq1gu-nadU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metadata management in Dataplex Universal Catalog\n",
        "from IPython.display import Image\n",
        "\n",
        "# Print the title/description\n",
        "print(\"The following diagram shows how Dataplex Universal Catalog scans data to report on statistical characteristics.\")\n",
        "print(\">>> RECOMMENDED >>> Use the Data Profile Scan or the Data Quality scan Module in the Dataplex Universal Catalog\\n\")\n",
        "print(\"please see here -> https://docs.cloud.google.com/dataplex/docs/data-profiling-overview\\n\")\n",
        "Image(url='https://docs.cloud.google.com/static/dataplex/images/data-profile-scan.png', width=800)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "6rb9u70ckUKx",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764905649768,
          "user_tz": 300,
          "elapsed": 666,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5453fc90-0bc6-496e-8fb5-0b4c67601139"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following diagram shows how Dataplex Universal Catalog scans data to report on statistical characteristics.\n",
            ">>> RECOMMENDED >>> Use the Data Profile Scan or the Data Quality scan Module in Dataplex Universal Catalog\n",
            "\n",
            "please see here -> https://docs.cloud.google.com/dataplex/docs/data-profiling-overview\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://docs.cloud.google.com/static/dataplex/images/data-profile-scan.png\" width=\"800\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Configuration --- ###\n",
        "# TODO(developer): Set the variable DATA SCAN regional location.\n",
        "# A Data Scan is a regional resource in Dataplex whose location can be different from the BigQuery dataset location.\n",
        "dataScanRegion = \"us-central1\"\n",
        "\n",
        "# Function\n",
        "def existsDataScan(parent_path: str):\n",
        "  \"\"\"Checks to see if a Data Scan of any type already exists\"\"\"\n",
        "\n",
        "  # Gather existing data scans\n",
        "  # https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.dataScans/list\n",
        "\n",
        "  url = f\"https://dataplex.googleapis.com/v1/{parent_path}/dataScans\"\n",
        "\n",
        "  # Gather existing data scans\n",
        "  json_result = restAPIHelper(url, \"GET\", None)\n",
        "  total_dataScans = 0\n",
        "\n",
        "  # Test to see if data scan exists, if so return\n",
        "  if \"dataScans\" in json_result:\n",
        "    for item in json_result[\"dataScans\"]:\n",
        "      total_dataScans += 1\n",
        "      dataScanId = item[\"name\"].split(\"/\")[-1]\n",
        "\n",
        "    print(f\"Data Scan(GET) json_result: {PrettyPrintJson(json.dumps(json_result))}\")\n",
        "\n",
        "    if total_dataScans > 0:\n",
        "            print(f\"\\nThere are {total_dataScans} Data Scan(s) created in the project {PROJECT_ID}.\")\n",
        "  else:\n",
        "      print(\"\\nThere are no Data Scan(s) created in the project {PROJECT_ID}.\")\n",
        "\n",
        "# Execute the function\n",
        "existsDataScan(f\"projects/{PROJECT_ID}/locations/{dataScanRegion}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D78AgLzKncz_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764917878311,
          "user_tz": 300,
          "elapsed": 308,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "03b66208-be56-4894-aab8-995010b74cd7"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Scan(GET) json_result: {\n",
            "  \"dataScans\": [\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us-central1/dataScans/ab3e23ec9-4182-4fbb-896c-f6502a74e87e\",\n",
            "      \"uid\": \"6f7893a0-a2f6-4e51-a348-fbfb153ec84a\",\n",
            "      \"description\": \"Profile scan for table \\\"\\\" with default configuration\",\n",
            "      \"displayName\": \"-default-profile\",\n",
            "      \"state\": \"ACTIVE\",\n",
            "      \"createTime\": \"2025-03-20T17:01:56.591575279Z\",\n",
            "      \"updateTime\": \"2025-03-20T17:02:01.957395870Z\",\n",
            "      \"data\": {\n",
            "        \"resource\": \"//bigquery.googleapis.com/projects/prj-dataplex-hcv/datasets/order_raw_zone/tables/order_raw_data\"\n",
            "      },\n",
            "      \"executionSpec\": {\n",
            "        \"trigger\": {\n",
            "          \"onDemand\": {}\n",
            "        }\n",
            "      },\n",
            "      \"executionStatus\": {\n",
            "        \"latestJobStartTime\": \"2025-03-20T17:02:37.679Z\",\n",
            "        \"latestJobEndTime\": \"2025-03-20T17:03:05.786252574Z\",\n",
            "        \"latestJobCreateTime\": \"2025-03-20T17:02:37.104428331Z\"\n",
            "      },\n",
            "      \"type\": \"DATA_PROFILE\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us-central1/dataScans/movie-dq-check\",\n",
            "      \"uid\": \"c647b7fd-d79d-4290-8ba4-c7cd43fbada6\",\n",
            "      \"displayName\": \"MOVIE_DQ_CHECK\",\n",
            "      \"state\": \"ACTIVE\",\n",
            "      \"createTime\": \"2025-03-20T06:42:55.959658709Z\",\n",
            "      \"updateTime\": \"2025-03-20T06:43:01.292676667Z\",\n",
            "      \"data\": {\n",
            "        \"entity\": \"projects/944496412289/locations/us-central1/lakes/movie-lake/zones/imdb-raw-zone/entities/title_basics\"\n",
            "      },\n",
            "      \"executionSpec\": {\n",
            "        \"trigger\": {\n",
            "          \"onDemand\": {}\n",
            "        }\n",
            "      },\n",
            "      \"executionStatus\": {\n",
            "        \"latestJobStartTime\": \"2025-03-20T06:44:04.964Z\",\n",
            "        \"latestJobEndTime\": \"2025-03-20T06:44:36.168115895Z\",\n",
            "        \"latestJobCreateTime\": \"2025-03-20T06:44:04.563529627Z\"\n",
            "      },\n",
            "      \"type\": \"DATA_QUALITY\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us-central1/dataScans/a537aa5c3-5df6-4968-b2a6-bcf5b9fc1181\",\n",
            "      \"uid\": \"a0ad39c7-df84-4b40-b1a0-f6be20fe5feb\",\n",
            "      \"description\": \"Data documentation scan for the table - \\\"trips\\\" with default configuration created through generate insights\",\n",
            "      \"displayName\": \"trips-data-documentation-scan\",\n",
            "      \"state\": \"ACTIVE\",\n",
            "      \"createTime\": \"2024-07-09T21:06:33.512410033Z\",\n",
            "      \"updateTime\": \"2024-07-09T21:06:38.614578221Z\",\n",
            "      \"data\": {\n",
            "        \"resource\": \"//bigquery.googleapis.com/projects/prj-dataplex-hcv/datasets/demo_dataset/tables/trips\"\n",
            "      },\n",
            "      \"executionSpec\": {\n",
            "        \"trigger\": {\n",
            "          \"onDemand\": {}\n",
            "        }\n",
            "      },\n",
            "      \"executionStatus\": {\n",
            "        \"latestJobStartTime\": \"2024-07-09T21:07:15.314149826Z\",\n",
            "        \"latestJobEndTime\": \"2024-07-09T21:07:35.561886395Z\",\n",
            "        \"latestJobCreateTime\": \"2024-07-09T21:07:15.314092636Z\"\n",
            "      },\n",
            "      \"type\": \"DATA_DOCUMENTATION\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us-central1/dataScans/a0fbda905-4b0b-4100-84f1-5784d60a3a24\",\n",
            "      \"uid\": \"4b6f2641-d7f8-4908-8ab1-b754efe78f8d\",\n",
            "      \"description\": \"Profile scan for table \\\"prj-dataplex-hcv:imdb_raw.names\\\" with default configuration\",\n",
            "      \"displayName\": \"prj-dataplex-hcv:imdb_raw.names-default-profile\",\n",
            "      \"state\": \"ACTIVE\",\n",
            "      \"createTime\": \"2025-05-28T20:00:02.836326205Z\",\n",
            "      \"updateTime\": \"2025-05-28T20:00:07.913152500Z\",\n",
            "      \"data\": {\n",
            "        \"resource\": \"//bigquery.googleapis.com/projects/prj-dataplex-hcv/datasets/imdb_raw/tables/names\"\n",
            "      },\n",
            "      \"executionSpec\": {\n",
            "        \"trigger\": {\n",
            "          \"onDemand\": {}\n",
            "        }\n",
            "      },\n",
            "      \"executionStatus\": {\n",
            "        \"latestJobStartTime\": \"2025-05-28T20:00:48.361Z\",\n",
            "        \"latestJobEndTime\": \"2025-05-28T20:01:19.508518734Z\",\n",
            "        \"latestJobCreateTime\": \"2025-05-28T20:00:46.919394942Z\"\n",
            "      },\n",
            "      \"type\": \"DATA_PROFILE\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us-central1/dataScans/scan-diet-nutritional-data-1\",\n",
            "      \"uid\": \"d33ae8c0-cceb-4996-bb47-2208b5d8088d\",\n",
            "      \"displayName\": \"scan_diet_nutritional_data_1\",\n",
            "      \"state\": \"ACTIVE\",\n",
            "      \"createTime\": \"2025-06-05T22:22:43.188456075Z\",\n",
            "      \"updateTime\": \"2025-06-05T22:22:48.550127746Z\",\n",
            "      \"data\": {\n",
            "        \"resource\": \"//bigquery.googleapis.com/projects/prj-dataplex-hcv/datasets/or_tools/tables/diet_nutritional_data\"\n",
            "      },\n",
            "      \"executionSpec\": {\n",
            "        \"trigger\": {\n",
            "          \"onDemand\": {}\n",
            "        }\n",
            "      },\n",
            "      \"executionStatus\": {\n",
            "        \"latestJobStartTime\": \"2025-06-05T22:23:24.893Z\",\n",
            "        \"latestJobEndTime\": \"2025-06-05T22:23:52.121312065Z\",\n",
            "        \"latestJobCreateTime\": \"2025-06-05T22:23:24.447344670Z\"\n",
            "      },\n",
            "      \"type\": \"DATA_PROFILE\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us-central1/dataScans/a839e5d7d-be12-49e6-9536-1fdc4e4ff23f\",\n",
            "      \"uid\": \"a36dde72-8fd1-4ad1-a921-40a950a935a0\",\n",
            "      \"description\": \"Data documentation scan for the table - \\\"product_feed\\\" with default configuration created through generate insights\",\n",
            "      \"displayName\": \"product_feed-data-documentation-scan\",\n",
            "      \"state\": \"ACTIVE\",\n",
            "      \"createTime\": \"2024-05-15T01:14:25.981126206Z\",\n",
            "      \"updateTime\": \"2024-05-15T01:14:30.831194116Z\",\n",
            "      \"data\": {\n",
            "        \"resource\": \"//bigquery.googleapis.com/projects/prj-dataplex-hcv/datasets/data_preparation_dataset/tables/product_feed\"\n",
            "      },\n",
            "      \"executionSpec\": {\n",
            "        \"trigger\": {\n",
            "          \"onDemand\": {}\n",
            "        }\n",
            "      },\n",
            "      \"executionStatus\": {\n",
            "        \"latestJobStartTime\": \"2024-05-15T01:56:18.683669374Z\",\n",
            "        \"latestJobEndTime\": \"2024-05-15T01:56:37.675901420Z\",\n",
            "        \"latestJobCreateTime\": \"2024-05-15T01:56:18.683622404Z\"\n",
            "      },\n",
            "      \"type\": \"DATA_DOCUMENTATION\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us-central1/dataScans/movie-title-scan\",\n",
            "      \"uid\": \"3c1e29ee-81ca-4fdb-bb3c-43eaee073674\",\n",
            "      \"displayName\": \"MOVIE_TITLE_SCAN\",\n",
            "      \"state\": \"ACTIVE\",\n",
            "      \"createTime\": \"2025-03-20T06:19:25.846762934Z\",\n",
            "      \"updateTime\": \"2025-03-20T06:19:30.992772615Z\",\n",
            "      \"data\": {\n",
            "        \"entity\": \"projects/944496412289/locations/us-central1/lakes/movie-lake/zones/imdb-raw-zone/entities/title_basics\"\n",
            "      },\n",
            "      \"executionSpec\": {\n",
            "        \"trigger\": {\n",
            "          \"onDemand\": {}\n",
            "        }\n",
            "      },\n",
            "      \"executionStatus\": {\n",
            "        \"latestJobStartTime\": \"2025-03-20T06:20:50.640Z\",\n",
            "        \"latestJobEndTime\": \"2025-03-20T06:21:23.118617133Z\",\n",
            "        \"latestJobCreateTime\": \"2025-03-20T06:20:50.112975095Z\"\n",
            "      },\n",
            "      \"type\": \"DATA_PROFILE\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"projects/prj-dataplex-hcv/locations/us-central1/dataScans/orders-dq-scan1\",\n",
            "      \"uid\": \"03e895ac-44dc-4159-bee1-e6965f9a92a9\",\n",
            "      \"displayName\": \"orders-dq-scan1\",\n",
            "      \"state\": \"ACTIVE\",\n",
            "      \"createTime\": \"2025-03-20T17:21:16.635507754Z\",\n",
            "      \"updateTime\": \"2025-03-20T17:21:22.350410582Z\",\n",
            "      \"data\": {\n",
            "        \"resource\": \"//bigquery.googleapis.com/projects/prj-dataplex-hcv/datasets/order_raw_zone/tables/order_raw_data\"\n",
            "      },\n",
            "      \"executionSpec\": {\n",
            "        \"trigger\": {\n",
            "          \"onDemand\": {}\n",
            "        }\n",
            "      },\n",
            "      \"executionStatus\": {\n",
            "        \"latestJobStartTime\": \"2025-03-20T17:22:00.103Z\",\n",
            "        \"latestJobEndTime\": \"2025-03-20T17:22:29.449829132Z\",\n",
            "        \"latestJobCreateTime\": \"2025-03-20T17:21:59.640892926Z\"\n",
            "      },\n",
            "      \"type\": \"DATA_QUALITY\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "There are 8 Data Scan(s) created in the project prj-dataplex-hcv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a BigQuery table into a bigframe\n",
        "\n",
        "This section uses **SQL** to run a manual and customized **data profile** for a table in BigQuery, however for a deeper data profile analysis, it is **strongly recommended** to use the the **data profile scan module** available in the **Dataplex Universal Catalog**.\n",
        "\n",
        "For more information, please see here -> https://docs.cloud.google.com/dataplex/docs/data-profiling-overview"
      ],
      "metadata": {
        "id": "f7QzvKg_58aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### --- Configuration --- ###\n",
        "# TODO(developer): Set the TABLE_ID to run a manual SQL data profile for. The table should be part of the DATASET_ID defined at the beginning.\n",
        "TABLE_ID = \"customer_sample_data\""
      ],
      "metadata": {
        "id": "L5angXeeTywW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764919057088,
          "user_tz": 300,
          "elapsed": 537,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full table reference for `bigframes`\n",
        "# *** PROJECT_ID, DATASET_ID, and TABLE_ID should be defined previously ***\n",
        "FULL_TABLE_REF = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
        "\n",
        "#set project id using bigframe option\n",
        "bf.options.bigquery.project=PROJECT_ID\n",
        "\n",
        "# Initialize BigQuery client for direct SQL (e.g., INFORMATION_SCHEMA)\n",
        "bq_client = bigquery.Client()\n",
        "\n",
        "print(f\"Connected to BigQuery via bigframes.\")\n",
        "\n",
        "# Load the BigQuery table into a bigframes DataFrame\n",
        "try:\n",
        "    df = bf.read_gbq_table(FULL_TABLE_REF)\n",
        "    print(f\"\\nSuccessfully loaded BigQuery table {FULL_TABLE_REF} into bigframes DataFrame. Shape: {df.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nError loading table: {e}\")\n",
        "    print(\"\\nPlease ensure the table {FULL_TABLE_REF} exists and you have the necessary permissions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "LueU-vqK6DA0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764919134747,
          "user_tz": 300,
          "elapsed": 929,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "3ea9751c-015a-4c03-b823-63cef36c6f4f"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to BigQuery via bigframes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "âœ… Completed. \n",
              "    Query processed 0 Bytes in a moment of slot time.\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully loaded BigQuery table prj-dataplex-hcv.demo_dataset.customer_sample_data into bigframes DataFrame. Shape: (10000, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [SQL] Table Overview (Row Count, Column Names, Data Types)\n",
        "We'll start by getting a high-level overview of the DataFrame, including its dimensions and column types."
      ],
      "metadata": {
        "id": "cFB5VQABlL-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n--- Table Overview: [{FULL_TABLE_REF}] ---\\n\")\n",
        "print(f\"\\nTotal rows: {df.shape[0]}\")\n",
        "print(\"\\nColumn information:\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "nVlaYOtxmDap",
        "outputId": "749623fa-194b-4577-a137-60a9cfead0ed",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764919143087,
          "user_tz": 300,
          "elapsed": 1816,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Table Overview: [prj-dataplex-hcv.demo_dataset.customer_sample_data] ---\n",
            "\n",
            "\n",
            "Total rows: 10000\n",
            "\n",
            "Column information:\n",
            "<class 'bigframes.dataframe.DataFrame'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "âœ… Completed. \n",
              "    Query processed 1.7 MB in a moment of slot time.\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "âœ… Completed. \n",
              "    Query processed 1.7 MB in a moment of slot time.\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 10000 entries, 0 to 9999\n",
            "Data columns (total 12 columns):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "âœ… Completed. \n",
              "    Query processed 1.7 MB in a moment of slot time.\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  #  Column             Non-Null Count    Dtype\n",
            "---  -----------------  ----------------  -------\n",
            "  0  customer_id        10000 non-null    string\n",
            "  1  first_name         10000 non-null    string\n",
            "  2  last_name          9500 non-null     string\n",
            "  3  email              9802 non-null     string\n",
            "  4  phone_number       9500 non-null     string\n",
            "  5  address            9500 non-null     string\n",
            "  6  city               10000 non-null    string\n",
            "  7  state              10000 non-null    string\n",
            "  8  zip_code           10000 non-null    string\n",
            "  9  registration_date  10000 non-null    string\n",
            " 10  total_spent        9501 non-null     Float64\n",
            " 11  customer_segment   10000 non-null    string\n",
            "dtypes: Float64(1), string(11)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "âœ… Completed. \n",
              "    Query processed 0 Bytes in a moment of slot time.\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memory usage: 1040000 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get descriptive statistics for numerical columns\n",
        "print(\"Get descriptive statistics for numerical columns\\n\")\n",
        "print(f\"Table: [{FULL_TABLE_REF}]\\n\")\n",
        "print(df.describe().to_pandas())\n",
        "\n",
        "# Check for null values\n",
        "print(\"Check for null values\\n\")\n",
        "print(df.isnull().sum().to_pandas())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "bFGfKmKRnZlm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764919147696,
          "user_tz": 300,
          "elapsed": 1725,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fb6665f5-eda3-4345-9bfd-9a0c77db935e"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get descriptive statistics for numerical columns\n",
            "\n",
            "Table: [prj-dataplex-hcv.demo_dataset.customer_sample_data]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "âœ… Completed. \n",
              "    Query processed 76.0 kB in a moment of slot time.\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       total_spent\n",
            "count       9501.0\n",
            "mean   2564.907397\n",
            "std    2617.505863\n",
            "min       -4977.77\n",
            "25%        1226.29\n",
            "50%        2492.39\n",
            "75%        3780.24\n",
            "max       90953.88\n",
            "Check for null values\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "âœ… Completed. \n",
              "    Query processed 1.7 MB in a moment of slot time.\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customer_id            0\n",
            "first_name             0\n",
            "last_name            500\n",
            "email                198\n",
            "phone_number         500\n",
            "address              500\n",
            "city                   0\n",
            "state                  0\n",
            "zip_code               0\n",
            "registration_date      0\n",
            "total_spent          499\n",
            "customer_segment       0\n",
            "dtype: Int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a customized Data Profile of a table in BigQuery by using SQL, Pandas and BigQuery Bigframe.\n",
        "# For a deeper data profile analysis, it is strongly recommended to use the the data profile scan module available in the Dataplex Universal Catalog.\n",
        "# Please see here for more information -> https://docs.cloud.google.com/dataplex/docs/data-profiling-overview\n",
        "\n",
        "def profile_bigquery_table():\n",
        "    \"\"\"\n",
        "    Connects to BigQuery, fetches data, and generates a detailed profile\n",
        "    for each column, including statistical calculations and anomaly detection.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the profiling results.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Setup BigQuery Client and Fetch Data\n",
        "    client = bigquery.Client(project=PROJECT_ID)\n",
        "    table_ref = f\"{FULL_TABLE_REF}\"\n",
        "    print(f\"Fetching data from table: {table_ref}\")\n",
        "\n",
        "    # Use a simple SELECT * to get all data (be mindful of large tables)\n",
        "    query = f\"SELECT * FROM `{table_ref}` LIMIT 100000\" # Limit rows for stability\n",
        "\n",
        "    # Fetch the data into a Pandas DataFrame\n",
        "    df = client.query(query).to_dataframe()\n",
        "\n",
        "    print(f\"Data fetched. DataFrame shape: {df.shape}\")\n",
        "\n",
        "    profiling_results = {}\n",
        "\n",
        "    # 2. Iterate Over Columns and Profile\n",
        "    for column in df.columns:\n",
        "        series = df[column]\n",
        "        dtype = series.dtype\n",
        "        results = {\"column_name\": column, \"dtype\": str(dtype)}\n",
        "\n",
        "        # --- Profiling Numeric Columns ---\n",
        "        if pd.api.types.is_numeric_dtype(series):\n",
        "            # Calculate descriptive statistics\n",
        "            results[\"type\"] = \"numeric\"\n",
        "            results[\"count\"] = series.count()\n",
        "            results[\"mean\"] = series.mean()\n",
        "            results[\"std_dev\"] = series.std()\n",
        "            results[\"min\"] = series.min()\n",
        "            results[\"max\"] = series.max()\n",
        "            results[\"quartiles\"] = series.quantile([0.25, 0.5, 0.75]).to_dict()\n",
        "\n",
        "            # Outlier Detection (Z-score)\n",
        "            mean = series.mean()\n",
        "            std = series.std()\n",
        "\n",
        "            # Avoid division by zero for columns with constant values\n",
        "            if std > 0:\n",
        "                z_scores = (series - mean) / std\n",
        "                # Identify outliers with |Z-score| > 3 (common threshold)\n",
        "                outliers = series[np.abs(z_scores) > 3].dropna()\n",
        "                results[\"outlier_count_z_score_3\"] = len(outliers)\n",
        "                results[\"outliers_sample\"] = outliers.unique()[:10].tolist()\n",
        "            else:\n",
        "                results[\"outlier_count_z_score_3\"] = 0\n",
        "                results[\"outliers_sample\"] = []\n",
        "\n",
        "        # --- Profiling String Columns ---\n",
        "        elif pd.api.types.is_object_dtype(series) or pd.api.types.is_string_dtype(series):\n",
        "            results[\"type\"] = \"string\"\n",
        "            results[\"non_null_count\"] = series.dropna().shape[0]\n",
        "            results[\"distinct_count\"] = series.nunique()\n",
        "\n",
        "            # Top Occurring Values\n",
        "            value_counts = series.value_counts().head(5)\n",
        "            results[\"top_values\"] = value_counts.to_dict()\n",
        "\n",
        "            # Inconsistent String Value Identification (Rare Occurrences)\n",
        "            # Find values that occur less than a low threshold (e.g., 2 times)\n",
        "            rare_threshold = 2\n",
        "            rare_values = value_counts[value_counts <= rare_threshold].index.tolist()\n",
        "            results[\"rare_value_count\"] = len(rare_values)\n",
        "            results[\"rare_values_sample\"] = rare_values[:10]\n",
        "\n",
        "        # --- Profiling Date/Time Columns ---\n",
        "        elif pd.api.types.is_datetime64_any_dtype(series) or pd.api.types.is_timedelta64_dtype(series):\n",
        "            results[\"type\"] = \"datetime/timestamp\"\n",
        "            results[\"non_null_count\"] = series.dropna().shape[0]\n",
        "            results[\"min_date\"] = series.min()\n",
        "            results[\"max_date\"] = series.max()\n",
        "\n",
        "            if series.min() is not pd.NaT and series.max() is not pd.NaT:\n",
        "                # Overall Time Range\n",
        "                time_range = series.max() - series.min()\n",
        "                results[\"overall_time_range\"] = str(time_range)\n",
        "\n",
        "                # Anomaly Detection (Dates far in the past/future)\n",
        "                # Define a reasonable range (e.g., between 1900 and the current year)\n",
        "                current_year = pd.Timestamp.now().year\n",
        "\n",
        "                # Check for dates outside a sensible range (e.g., before 1900 or 10 years in future)\n",
        "                anomaly_low = pd.to_datetime('1900-01-01')\n",
        "                anomaly_high = pd.to_datetime(f'{current_year + 10}-12-31')\n",
        "\n",
        "                anomalies = series[(series < anomaly_low) | (series > anomaly_high)].dropna()\n",
        "                results[\"anomaly_count_range\"] = len(anomalies)\n",
        "                results[\"anomaly_sample\"] = anomalies.unique()[:10].tolist()\n",
        "\n",
        "\n",
        "        # --- Default/Other Types (e.g., Boolean, Array, Struct) ---\n",
        "        else:\n",
        "            results[\"type\"] = \"other\"\n",
        "            results[\"non_null_count\"] = series.dropna().shape[0]\n",
        "            results[\"distinct_count\"] = series.nunique()\n",
        "\n",
        "        profiling_results[column] = results\n",
        "\n",
        "    return profiling_results\n",
        "\n",
        "# --- Example Usage ---\n",
        "profile_output = profile_bigquery_table()\n",
        "\n",
        "print(\"\\n*** SQL Data Profiling Results ***\")\n",
        "\n",
        "# # Print the results in a readable format\n",
        "for col_name, profile in profile_output.items():\n",
        "    print(\"\\n\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"## ðŸ“Š Column: {col_name} (Type: {profile['type']})\")\n",
        "    for key, value in profile.items():\n",
        "         if key not in [\"column_name\", \"type\"]:\n",
        "             # Pretty print pandas series/timestamps\n",
        "             if isinstance(value, pd.Timestamp):\n",
        "                  print(f\"  {key}: {value.isoformat()}\")\n",
        "             else:\n",
        "                 print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsgxlvarC3XG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1764920457545,
          "user_tz": 300,
          "elapsed": 1999,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "721d4461-e827-46cd-8e16-82988fbadfb3"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from table: prj-dataplex-hcv.demo_dataset.customer_sample_data\n",
            "Data fetched. DataFrame shape: (10000, 12)\n",
            "\n",
            "*** SQL Data Profiling Results ***\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: customer_id (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 10000\n",
            "  distinct_count: 9950\n",
            "  top_values: {'c15c63a0-8f29-40d1-a4fe-5fcf38a38349': 2, 'b474de16-2aa8-4c0c-b0c6-7963d51ec771': 2, '9c345889-9b84-46f6-b0eb-8907ec975c75': 2, '0cb4d754-5711-487b-ad63-ddcff5ee39e4': 2, '3f575b38-b74d-45a2-b36d-4d887cfa5631': 2}\n",
            "  rare_value_count: 5\n",
            "  rare_values_sample: ['c15c63a0-8f29-40d1-a4fe-5fcf38a38349', 'b474de16-2aa8-4c0c-b0c6-7963d51ec771', '9c345889-9b84-46f6-b0eb-8907ec975c75', '0cb4d754-5711-487b-ad63-ddcff5ee39e4', '3f575b38-b74d-45a2-b36d-4d887cfa5631']\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: first_name (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 10000\n",
            "  distinct_count: 845\n",
            "  top_values: {'Michael': 217, 'Jennifer': 159, 'James': 155, 'David': 145, 'John': 139}\n",
            "  rare_value_count: 0\n",
            "  rare_values_sample: []\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: last_name (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 9500\n",
            "  distinct_count: 988\n",
            "  top_values: {'Smith': 228, 'Johnson': 160, 'Jones': 128, 'Williams': 121, 'Brown': 115}\n",
            "  rare_value_count: 0\n",
            "  rare_values_sample: []\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: email (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 9802\n",
            "  distinct_count: 9527\n",
            "  top_values: {'jennifer_at_bad-domain.com': 5, 'michael_at_bad-domain.com': 3, 'brandon_at_bad-domain.com': 3, 'cjohnson@hotmail.com': 3, 'ejohnson@hotmail.com': 3}\n",
            "  rare_value_count: 0\n",
            "  rare_values_sample: []\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: phone_number (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 9500\n",
            "  distinct_count: 9500\n",
            "  top_values: {'149556933006722': 1, '(326)771-1973x62542': 1, '5513748833': 1, '+1-658-880-4715x43710': 1, '(825)622-7602x7697': 1}\n",
            "  rare_value_count: 5\n",
            "  rare_values_sample: ['149556933006722', '(326)771-1973x62542', '5513748833', '+1-658-880-4715x43710', '(825)622-7602x7697']\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: address (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 9500\n",
            "  distinct_count: 9500\n",
            "  top_values: {'86156 Corey Falls Suite 334': 1, '7557 Kathryn Dam': 1, '6826 Christina Manor': 1, '807 Peterson Locks Apt. 570': 1, '0444 Jordan Motorway Suite 200': 1}\n",
            "  rare_value_count: 5\n",
            "  rare_values_sample: ['86156 Corey Falls Suite 334', '7557 Kathryn Dam', '6826 Christina Manor', '807 Peterson Locks Apt. 570', '0444 Jordan Motorway Suite 200']\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: city (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 10000\n",
            "  distinct_count: 7814\n",
            "  top_values: {'New Michael': 11, 'Port Michael': 11, 'Lake John': 11, 'New David': 10, 'North Michael': 10}\n",
            "  rare_value_count: 0\n",
            "  rare_values_sample: []\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: state (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 10000\n",
            "  distinct_count: 89\n",
            "  top_values: {'OH': 193, 'AL': 191, 'PA': 190, 'NC': 188, 'KY': 187}\n",
            "  rare_value_count: 0\n",
            "  rare_values_sample: []\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: zip_code (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 10000\n",
            "  distinct_count: 9419\n",
            "  top_values: {'9021O': 35, '123456': 21, 'N/A': 18, 'ABCDE': 14, '1234': 12}\n",
            "  rare_value_count: 0\n",
            "  rare_values_sample: []\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: registration_date (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 10000\n",
            "  distinct_count: 2853\n",
            "  top_values: {'2021-06-13': 13, '2023-09-26': 13, '2024-10-12': 13, '2021-07-28': 12, '2025-03-24': 12}\n",
            "  rare_value_count: 0\n",
            "  rare_values_sample: []\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: total_spent (Type: numeric)\n",
            "  dtype: float64\n",
            "  count: 9501\n",
            "  mean: 2564.907397116093\n",
            "  std_dev: 2617.505863314915\n",
            "  min: -4977.77\n",
            "  max: 90953.88\n",
            "  quartiles: {0.25: 1243.88, 0.5: 2489.68, 0.75: 3787.32}\n",
            "  outlier_count_z_score_3: 10\n",
            "  outliers_sample: [58029.85, 57138.82, 76308.96, 57921.5, 70730.34, 90953.88, 50128.23, 83987.95, 74938.78, 58534.0]\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "## ðŸ“Š Column: customer_segment (Type: string)\n",
            "  dtype: object\n",
            "  non_null_count: 10000\n",
            "  distinct_count: 9\n",
            "  top_values: {'Bronze': 2524, 'Silver': 2461, 'Standard': 2432, 'Gold': 2383, 'Gld': 50}\n",
            "  rare_value_count: 0\n",
            "  rare_values_sample: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summary and next steps\n",
        "\n",
        "This notebook demonstrates how to verify **Data Governance** topics related to **BigQuery** Data in GCP by checking **IAM permissions and Policies**, and **Dataplex Universal Catalog** resources that exists in the project defined in the parameter **PROJECT_ID**.\n",
        "\n",
        "**Dataplex Universal Catalog** replaces **Data Catalog**, providing more complex metadata management, advanced data governance features, more powerful data search, less complex access control, and a unified API to ease development.\n",
        "\n",
        "The **Dataplex Universal Catalog** resources currently supported are: **Business Glossary (Glossary, GlossaryCategory, GlossaryTerm), and Entry Groups, Aspect Types, Entry Types and Entry(s)**. Finally, this notebook showcased how to leverage `bigframes` to perform a customized data profiling tasks directly within BigQuery by using SQL.\n",
        "\n",
        "As a next step, you can start using **Data Quality** and **Data Profling Scans** to avoid the use of extensive Python and SQL programming. **Dataplex Universal Catalog** lets you better understand the profile of your data by creating a **data profile scan**. A **data profile scan** is a type of **Dataplex Universal Catalog data scan** that analyzes a BigQuery table to generate statistical insights.\n",
        "\n",
        "For more information, please see here -> https://docs.cloud.google.com/dataplex/docs/data-profiling-overview\n",
        "\n"
      ],
      "metadata": {
        "id": "eD3PlbZC1iom"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "name": "Ensure data quality with BigFrames in BigQuery"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}